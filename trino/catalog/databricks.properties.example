# ============================================================================
# Databricks Unity Catalog Connector Configuration
# ============================================================================
# 
# INSTRUCTIONS:
# 1. Copy this file to: databricks.properties
# 2. Replace YOUR_WORKSPACE with your Databricks workspace URL
# 3. Replace YOUR_TOKEN with your Databricks Personal Access Token
# 4. DO NOT commit databricks.properties to Git (it's in .gitignore)
# ============================================================================

connector.name=delta_lake
hive.metastore=unity-catalog

# Databricks connection
unity-catalog.url=https://YOUR_WORKSPACE.cloud.databricks.com
unity-catalog.token=dapi_YOUR_TOKEN_HERE
unity-catalog.catalog=bristol_housing

# Delta Lake configuration
delta.register-table-procedure.enabled=true
delta.enable-non-concurrent-writes=true
delta.vacuum.enabled=true

# Performance tuning
hive.max-partitions-per-scan=100000
hive.max-partitions-per-writers=100
hive.max-initial-splits=200
hive.max-outstanding-splits=1000

# Caching
hive.metastore-cache-ttl=2h
hive.metastore-refresh-interval=10m
hive.metastore-cache-maximum-size=10000

# Compression
hive.compression-codec=SNAPPY

# Query optimization
hive.pushdown-filter-enabled=true
hive.partition-statistics-sample-size=100

# Timeouts
hive.metastore-timeout=2m
hive.s3.max-connections=500

# ============================================================================
# Azure ADLS Configuration (if using external location on Azure)
# ============================================================================
# Uncomment and configure if your Databricks uses Azure ADLS storage:
#
# hive.azure.abfs-storage-account=YOUR_STORAGE_ACCOUNT
# hive.azure.abfs-access-key=YOUR_ACCESS_KEY